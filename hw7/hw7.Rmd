---
title: "hw7"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = T)
library(nnet)
```

## Part 1 – classifying instances using ANN  

The ANN to the write has 3 input nodes (1, 2 and 3), 2 nodes in the hidden layer (1 and 2) and the output node. Assume that the threshold function in the hidden layer is a step function that output 1 if the sum of its inputs is >= 0 and -1 if the sum is less than zero.  There is no threshold function for the output layer – it simply outputs the sum of its inputs.  

Calculate the output for the following inputs  

1.	{1, 1, 1}  

**Answer: **  

1 * (-0.7) + 1 * 0.3 + 1 * 0.5 = 0.1 > 0, so hiden.node1 = 1  

1 * (0.8) + 1 * (-0.1) + 1 * 0.3 = 1 > 0, so hiden.node2 = 1  

1 * 0.9 + 1 * 0.5 = 1.4


2.	{1, 5, -3}  

**Answer: **  

1 * (-0.7) + 5 * 0.3 + (-3) * 0.5 = -0.7 < 0, so hiden.node1 = -1  

1 * (0.8) + 5 * (-0.1) + (-3) * 0.3 = -0.6 < 0, so hiden.node2 = -1  

(-1) * 0.9 + (-1) * 0.5 = -1.4


## Part 2 – creating a perceptron in R  


For this part, you are to write two functions that work together to simulate a single perceptron.  

•	The first function will perform the task of one iteration of calculating the weights.  The parameters will be the data, initial weights and the learning rate.  You will be using the formula from the lecture slides.  Your function should calculate all of the changes to the weights first and return the new weights.  It is important to keep two sets of weights, so that you are always using the prior weights when calculating the new weights.  You should also use the matrix functions (like multiplication and dot product) instead of using loops.  

•	The second function will use a while loop to call the first function and update the weights.  The stopping condition is when the weights do not change (you can compare the sum of new vs. the old weights).  For your initial weights, use 0.1 for all weights and use 0.1 for the training rate.  The file hw07dataTiny.txt contains the training set.  You may find it useful to use the R functions sign, colSums and the matrix/vector multiplication and dot product operators.  

```{r}
data.training <- read.table("hw07dataTiny.txt", header = T)
#wgts <- c(rep(0.1, 4))
#data.training.with.bias <-
#  as.matrix(cbind(1.0, hw07.data.tiny[, 1:3]))
#sign(data.training.with.bias %*% wgts)
#val <-
#  0.1 * (data.training[, 4] - sign(data.training.with.bias %*% wgts))
#c(val) * data.training.with.bias
#colSums(c(val) * data.training.with.bias)

get.adjustments <- function(training.data) {
  training.data.length <- length(training.data)
  wgts <- c(rep(0.1, training.data.length))
  training.data.with.bias <-
    as.matrix(cbind(1.0, training.data[, 1:(training.data.length - 1)]))
  val <-
    0.1 * (training.data[, 4] - sign(training.data.with.bias %*% wgts))
 colSums(c(val) * training.data.with.bias)
}

get.adjustments(data.training)
#while (condition) {
  
#}
```

## Part 3 – building models in R  

You have been provided a data set with many instances with 5 variables – 4 numeric and 1 class.  The data set is separated into a training set, hw07dataTrain.txt and a test set hw07dataTest.txt.  

Follow these directions to process the data, create models and examine the results:  

1.	Use nnet to build a model with the training set.  Use 3 hidden nodes and be sure to set.seed(1) before each time you create the model.  
\
```{r}
hw7TrainData <- read.table("hw07dataTrain.txt", header = T)
hw7TestData <- read.table("hw07dataTest.txt", header = T)

set.seed(1)
mod <-
  nnet(
    class ~ var1 + var2 + var3 + var4,
    data = hw7TrainData,
    size = 4,
    type = "class",
    trace = F,
    wgts = .1
  )
```

2.	Predict the results using the test set.  Create a confusion matrix using the table command. 
\
```{r}
pred <- as.integer(predict(mod, hw7TestData))
table(pred, hw7TestData[, 5])
```

