---
title: "HW6"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = T)

library(e1071)
library(tidyverse)
library(rpart)
library(caret)
```

Part 1 – model building

Read the supplied data set hw06data1.txt into R.  This is a generic, sample data set with 2 attribute, x1 and x2 and the class.  Class values are -1 and 1.  The data values can be (nearly) linearly separated.  You are asked to determine the support vectors – that is the data values from both classes that lie on the separating lines.  You should identify 4 support vectors in this case.  Additionally, you should also identify the 2 data points that are on the wrong side of the support vectors and need slack variables to be placed on the correct side of the line.  

It is recommended that you plot the data with different markers or colors for the two classes and then save the plot as a graphics file (png for example).  Then upload the file into word or a graphics program and use a line drawing tool to visualize the line separating the two classes.


\
```{r}
data1 <- read.table("hw06data1.txt", header = T)

data1[, 3] <- as.factor(data1[, 3])

ggplot(data1, aes(x = x1, y = x2, color = cls)) + geom_point()

mod1 <- svm(data1$cls ~ ., data1, kernel = "linear", scale = F)
# mod2 <- svm(data1[, 1:2], data1[, 3], kernel = "linear")

summary(mod1)
#summary(mod2)

plot(x = mod1, data1, x2~x1,grid=300)
#plot(x = mod2, data1)

sv_data <- data1 %>% slice(mod1$index)
sv_data

ggplot(data1, aes(x = x1, y = x2, color = cls)) +
  geom_point() +geom_point(data=sv_data, aes(x=x1,y=x2), color="purple", size=4, alpha=0.5) 

# weight vector
w <- t(mod1$coefs) %*% mod1$SV

# slope
slope1 <- -w[1]/w[2]

# intercept
intercept1 <- mod1$rho/w[2]

ggplot(data1, aes(x = x1, y = x2, color = cls)) +
  geom_point() +geom_point(data=sv_data, aes(x=x1,y=x2), color="purple", size=4, alpha=0.5)  + 
  geom_abline(slope = slope1, intercept = intercept1) + 
  geom_abline(slope = slope1, intercept = intercept1-1/w[2], linetype="dashed") + 
  geom_abline(slope = slope1, intercept = intercept1+1/w[2], linetype="dashed") 
```

**Answer:**  

1. Support vectors

|x1|x2|
|----|----|
|3.0|2.9|
|0.9|1.4|
|7.3|3.3|
|10.0|5.0|  

2. Data points needing slack variables  

|x1|x2|
|----|----|
|6.8|3.2|
|10.6|5.9|  

3. Predict the class for the instance, {8.0, 5.1, ?}  


\
```{r}
test_m <- data.frame(matrix(c(8.0, 5.1), nrow = 1))
colnames(test_m) <- c("x1", "x2")
predict(mod1, test_m)
```

class = -1


Part 2 – using SVM for a real data set. For this part you will be using the data in the file hw06data2.csv which was downloaded from publicly available data sources*.  This is a table from the Lincoln Nebraska’s police department of the arrests that resulted in “use of control”.  There should also be a codebook posted to explain the attributes.  

Read the table into R and extract just the columns: ofcage, ofcrace, ofcgender, susage, susrace, susgender, alcohol, drugs, mental and susinjury.  The column susinjury will be the class.  Now select only the instances where ofcrace is 1 or 2, susrace is 1 or 2 and the susinjury is 0 or 1.  The resulting table should be 312 instances and 10 attributes (including the class).  You will be using decision trees (rpart) and svm to build models for this data.  To use the tree it is best to make all attributes factors except the ages.  For svm you will have to make them all numeric except the class.  

**Answer:**  

\
```{r}
data2 <- read.csv("hw06data2.csv")
data2 <-
  data2[, c(
    "ofcage",
    "ofcrace",
    "ofcgender",
    "susage",
    "susrace",
    "susgender",
    "alcohol",
    "drug",
    "mental",
    "susinjury"
  )]
new_data2 <-
  data2 %>% filter(ofcrace %in% (1:2) &
                     susrace %in% (1:2) & susinjury %in% (0:1))

new_data2[, "susage"] <- as.integer(new_data2[, "susage"])
new_data2[, "susinjury"] <- as.integer(new_data2[, "susinjury"])

data2_factored <- new_data2
for (i in c(2:3, 5:10)) {
  data2_factored[, i] <- as.factor(data2_factored[, i])
}

data2_integerized <- new_data2
data2_integerized[,"susinjury"] <- as.factor(data2_integerized[,"susinjury"]) 
```


Create the model for rpart and then 2 for svm, 1 that is a linear kernel and the other a polynomial.  For all three create a random partition where 2/3s of the data is for training and the other 1/3 is for testing.  After you build the models, predict the results with the test data.  Then report the accuracy for all three and answer the questions on the handin sheet. 

\
```{r}
#set.seed(1)

training.fac.row <- sample(nrow(data2_factored)) %% 3 != 0
training.fac <- data2_factored[training.fac.row,]
testing.fac.row <- sample(nrow(data2_factored)) %% 3 == 0
testing.fac <- data2_factored[testing.fac.row,]

training.int.row <- sample(nrow(data2_integerized)) %% 3 != 0
training.int <- data2_integerized[training.int.row,]
testing.int.row <- sample(nrow(data2_integerized)) %% 3 == 0
testing.int <- data2_integerized[testing.int.row,]

tree.mod <- rpart(susinjury ~ ., training.fac)
svm.mod.linear <-
  svm(training.int$susinjury ~ ., training.int, kernel = "linear")
svm.mod.poly <-
  svm(training.int$susinjury ~ ., training.int, kernel = "polynomial")

pred.tree <- predict(tree.mod, testing.fac, type = "class")
pred.svm.linear <- predict(svm.mod.linear, testing.int)
pred.svm.poly <- predict(svm.mod.poly, testing.int)

matr.rpart <- confusionMatrix(pred.tree, testing.fac$susinjury)
matr.svm.linear <- confusionMatrix(pred.svm.linear, testing.int$susinjury)
matr.svm.poly <- confusionMatrix(pred.svm.poly, testing.int$susinjury)
```

**Answer:**  

1. confusion matrices
\
```{r}
# rpart
matr.rpart$table
# svm linear
matr.svm.linear$table
#svm poly
matr.svm.poly$table
```


2. accuracy
\
```{r}
# rpart
matr.rpart$overall[1]
# svm linear
matr.svm.linear$overall[1]
# svm poly
matr.svm.poly$overall[1]
```

