---
title: "CIS 635 Project"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output: 
  pdf_document:
    toc: yes
    toc_depth: '2'
  github_document:
  html_document:
    fig_caption: yes
    theme: lumen
    toc: yes
    toc_depth: 2
    df_print: kable
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  cache.lazy = FALSE,
  dpi = 180,
  fig.width = 8,
  fig.height = 5
)
library(tidyverse)
library(tidymodels)
library(skimr)
library(caret)
library(stacks)
theme_set(theme_minimal())
```
# Introduction  

In this project, we are given some demographic information and simple measurements of patients, and their test results for a certain disease. With these data, we practice some of the data mining techniques we learned through this class, including decision trees, naive Bayes, KNN, SVM, and ANN.  

***

# Preprocessing  

We first load the data into R.  

```{r}
personal_data <- read.table("projData1a.txt", header = T)
test_data <- read.table("projData1b.txt", header = T)
```

We take a look at the summary.  

```{r}
skim_without_charts(personal_data)
skim_without_charts(test_data)
```

We see a number of factor variables and "heartRate" has 738 missing values. For "weight", "height", and "heartRate", we do not see any extreme values. 

We convert the categorical variables to factors and merge the two sets by id.  

```{r}
personal_data_factored <-
  personal_data %>%
  mutate(across(c(ethnic:gender), as.factor))

test_data_factored <- 
  test_data %>% 
  mutate(disease = as.factor(disease))

merged_factored <-
  merge(personal_data_factored, test_data_factored, by = "id")
```

738 is less than 4% of the total records. We could either ignore the records with NA values for "heartRate", or we could replace these missing fields with values. As an exercise, we try replacing the missing fields with some appropriate measurements.  

We explore "heartRate" and its relationship with other variables in the dataset, in order to determine the proper replacements.  

```{r}
merged_factored %>%
  dplyr::select(c(age, weight:heartRate, testA:testE)) %>%
  pivot_longer(c(age, weight:height, testA:testE)) %>%
  ggplot(aes(heartRate, value)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap(~ name, scales = "free_y")

merged_factored %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::rearrange() %>%
  corrr::shave() %>%
  corrr::rplot(shape = 15, colours = c("darkorange","white","darkcyan"))

merged_factored %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::network_plot(min_cor = 0.2)
```

From the plot for numeric variables, we do not see stronger trends except for "testE" and "weight." In other words, "heartRate" is not strongly associated with most numerical variables. We take a look at the categorical variables.  

```{r}
merged_factored %>%
  dplyr::select(c(ethnic:gender, heartRate, disease)) %>%
  pivot_longer(c(ethnic:gender, disease)) %>%
  ggplot(aes(heartRate, value, fill = value)) +
  geom_boxplot() +
  facet_wrap(~ name, scales = "free_y") 
```

In the plots for categorical variables and , we find the variables "ethnic" and "gender" are worth looking into, because their plots show greater variations of heartRate for different categories. We take a closer look at "gender" and "ethnic".  

```{r}
merged_factored %>%
  dplyr::select(c(ethnic, gender, heartRate)) %>%
  pivot_longer(c(ethnic, gender)) %>%
  ggplot(aes(heartRate, value, fill = value)) +
  geom_boxplot() +
  facet_wrap( ~ name, scales = "free_y") 

merged_factored %>%
  group_by(gender) %>%
  summarise(
    mean_heartRate = mean(heartRate, na.rm = TRUE),
    median_heartRate = median(heartRate, na.rm = TRUE)
  )

merged_factored %>%
  group_by(ethnic) %>%
  summarise(
    mean_heartRate = mean(heartRate, na.rm = TRUE),
    median_heartRate = median(heartRate, na.rm = TRUE)
  )
```

The mean "heartRate" values clearly differ between the genders, and between two ethnic groups and the other four.  

```{r}
merged_factored %>%
  ggplot(aes(ethnic, heartRate, fill = gender)) +
  geom_boxplot()

merged_factored %>%
  group_by(ethnic, gender) %>%
  summarise(
    mean_heartRate = mean(heartRate, na.rm = TRUE),
    median_heartRate = median(heartRate, na.rm = TRUE)
  ) 
```

We take a closer look and find 75 might be a good "heartRate" candidate for both genders of ethnic group 1 and 2. For the other ethnic groups, 60 looks like a good approximation for gender 0 and 67 for gender 1. Next we assign the miss values as such.  

```{r}
merged_factored_no_na <- merged_factored %>%
  na.omit()

filled_1 <- merged_factored %>%
  filter(ethnic == 1 | ethnic == 2) %>%
  filter(is.na(heartRate)) %>%
  mutate(heartRate = replace(heartRate, values = 75))

filled_2 <- merged_factored %>%
  filter(!(ethnic == 1 | ethnic == 2)) %>%
  filter(is.na(heartRate)) %>%
  filter(gender == 0) %>%
  mutate(heartRate = replace(heartRate, values = 60))

filled_3 <- merged_factored %>%
  filter(!(ethnic == 1 | ethnic == 2)) %>%
  filter(is.na(heartRate)) %>%
  filter(gender == 1) %>%
  mutate(heartRate = replace(heartRate, values = 67))

merged_factored_NAfilled <-
  rbind(merged_factored_no_na,
        filled_1,
        filled_2,
        filled_3)

project_data_df <-
  merged_factored_NAfilled %>%
  dplyr::select(-id)
```

The "recipe" package of "tidymodels" provides a number of methods for filling in missing values. We compare the results from our above operations with step_knnimpute, which replaces missing values using a k-nearest neighbor approach.  

```{r}
knn_impute <- recipe(disease ~ ., merged_factored) %>% 
  step_knnimpute(heartRate, neighbors = 3) %>% 
  prep %>% 
  juice

original_sum <- summary(merged_factored$heartRate)

transformed_sum <- summary(project_data_df$heartRate)

knn_impute_sum <- summary(knn_impute$heartRate)

sum_table <- bind_rows(original_sum,
                       transformed_sum,
                       knn_impute_sum)

bind_cols(sum_table, "source" = c("original",
                                  "transformed",
                                  "knn_impute"))

```

We see the difference between our replacement values for "heartRate" and that from "step_impute" is minimal for this dataset. 

***  

# Overview  

With data now cleaned and merged, we take another look at our data.  

```{r}
project_data_df %>%
  dplyr::select(c(age, weight:disease)) %>%
  pivot_longer(c(age, weight:testE)) %>%
  ggplot(aes(disease, value)) +
  geom_boxplot() +
  facet_wrap( ~ name, scales = "free_y")

project_data_df %>%
  dplyr::select(c(ethnic:gender, disease)) %>%
  pivot_longer(ethnic:gender) %>%
  ggplot(aes(disease, value, col = value, fill = value)) +
  geom_bar(stat = "identity") +
  facet_wrap( ~ name, scales = "free_y")

project_data_df %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=key), color="black") +
  facet_wrap(~ key, scales = "free")
```

The mean difference for "heartRate" seem to vary at a greater extent for disease. The numerical variables are mostly normal, except maybe "weight".  

***  

# Model Training

We start by dividing the data into a train and a test set. By default, this creates a 3 to 1 split between a training set and a testing set.  The "strata" parameter keeps the proportion of "disease" as in the whole set. The "vfold_cv" method creates 10 folds for resampling.  

```{r}

set.seed(1)
# split data into train and test sets
data_split <- initial_split(project_data_df, strata = disease)
train_split <- training(data_split)
test_split <- testing(data_split)

set.seed(1)
train_cv <- vfold_cv(train_split)
model_control <- control_grid(save_pred = TRUE)
```

## Decision tree    

We set up the pipeline for training the decision tree model. Parameters "cost_complexity", "tree_depth", and "min_n" are tuned on a simple grid of values. The final model parameters are selected based on "accuracy".  

The training procedure for other models follows a similar process, adjusting for relevant parameters.    

### Training  

```{r}
tree_rec <- 
  recipe(disease ~ ., train_split)

tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_workflow <- workflow() %>% 
  add_recipe(tree_rec) %>% 
  add_model(tree_spec)

tree_grid <- tree_spec %>% 
  parameters() %>% 
  grid_regular(levels = 4)

doParallel::registerDoParallel()

set.seed(1)
tree_rs <- tune_grid(
  tree_workflow,
  resamples = train_cv,
  grid = tree_grid,
  control = model_control
)

final_tree <- 
  tree_workflow %>% 
  finalize_workflow(select_best(tree_rs, "accuracy"))

fit_tree <- 
  last_fit(final_tree, data_split)

doParallel::stopImplicitCluster()

fit_tree %>%
  collect_metrics()

fit_tree %>%
  collect_predictions() %>%
  conf_mat(truth = disease, .pred_class)

final_tree %>%
  fit(data = train_split) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

### Result  



## Naive Bayes  

```{r}
nb_rec <- 
  recipe(disease ~ ., train_split)

nb_spec <- discrim::naive_Bayes(
  smoothness = tune(),
  Laplace = tune()
) %>%
  set_engine("klaR") 

nb_workflow <- workflow() %>% 
  add_recipe(nb_rec) %>% 
  add_model(nb_spec)

nb_grid <- nb_spec %>%
  parameters() %>%
  grid_regular(levels = 4)

doParallel::registerDoParallel()

set.seed(1)
nb_rs <- tune_grid(nb_workflow,
                   resamples = train_cv,
                   grid = nb_grid,
                   control = model_control)

final_nb <- 
  nb_workflow %>% 
  finalize_workflow(select_best(nb_rs, "accuracy"))

fit_nb <- 
  last_fit(final_nb, data_split)

doParallel::stopImplicitCluster()


fit_nb %>%
  collect_metrics()

fit_nb %>%
  collect_predictions() %>%
  conf_mat(truth = disease, .pred_class)
```

## KNN  

For knn, we convert factors to dummy variables using "step_dummy", and center and scale numeric values using "step_normalize".  

```{r}
knn_rec <-
  recipe(formula = disease ~ ., data = train_split) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  prep()


knn_spec <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_workflow <- workflow() %>% 
  add_recipe(knn_rec) %>% 
  add_model(knn_spec)

knn_grid <- grid_regular(neighbors(),
                         weight_func(),
                         dist_power(),
                         levels = 2)

doParallel::registerDoParallel()

set.seed(1)
knn_rs <- tune_grid(knn_workflow,
                   resamples = train_cv,
                   grid = knn_grid,
                   control = model_control)

doParallel::stopImplicitCluster()

final_knn <- 
  knn_workflow %>% 
  finalize_workflow(select_best(knn_rs, "accuracy"))

set.seed(1)
fit_knn <- 
  last_fit(final_knn, data_split)

fit_knn %>%
  collect_metrics()

fit_knn %>% 
  collect_predictions() %>% 
  conf_mat(truth = disease, .pred_class)
```

## SVM  

For svm, we convert factors to dummy variables using "step_dummy", and center and scale numeric values using "step_normalize".  

Parsnip now only supports poly and radial kernels. We choose poly to practice with.  

```{r}
svm_rec <-
  recipe(disease ~ ., train_split) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  prep()
  
svm_spec <- svm_poly(
  cost = 0.001,
  degree = 3
) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_workflow <- workflow() %>% 
  add_recipe(svm_rec) %>% 
  add_model(svm_spec)

doParallel::registerDoParallel()

set.seed(1)

svm_fit <- 
  svm_spec %>% 
  fit(disease ~., data = train_split)

svm_fit_rs <- svm_workflow %>% 
  fit_resamples(train_cv)

fit_svm <- svm_workflow %>%
  finalize_workflow(svm_fit_rs$.metrics) %>%
  last_fit(data_split)

#svm_test_pred <-
#  predict(svm_fit, test_split) %>%
#  bind_cols(predict(svm_fit, test_split, type = "prob")) %>%
#  bind_cols(test_split %>% select(disease))

doParallel::stopImplicitCluster()



#svm_test_pred %>%
#  accuracy(truth = disease, .pred_class)
#
#svm_test_pred %>%
#  roc_auc(truth = disease, .pred_class)

fit_svm %>% 
  collect_metrics()

fit_svm %>% 
  collect_predictions() %>% 
  conf_mat(truth = disease, .pred_class)
```

## ANN  

For ann, we convert factors to dummy variables using "step_dummy", and center and scale numeric values using "step_normalize".  

```{r}
ann_rec <-
  recipe(disease ~ ., train_split) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  prep()

ann_spec <- mlp(
  hidden_units = tune(),
  penalty = tune(),
  epochs = tune()
) %>%
  set_engine("nnet") %>%
  set_mode("classification")

ann_workflow <- workflow() %>% 
  add_recipe(ann_rec) %>% 
  add_model(ann_spec)

ann_grid <- ann_spec %>% 
  parameters() %>% 
  grid_regular(levels = 2)

doParallel::registerDoParallel()

set.seed(1)
ann_rs <- tune_grid(ann_workflow,
                   resamples = train_cv,
                   grid = ann_grid,
                   control = model_control)

final_ann <- 
  ann_workflow %>% 
  finalize_workflow(select_best(ann_rs, "accuracy"))

set.seed(1)
fit_ann <- 
  last_fit(final_ann, data_split)

doParallel::stopImplicitCluster()

fit_ann %>%
  collect_metrics()

fit_ann %>% 
  collect_predictions() %>% 
  conf_mat(truth = disease, .pred_class)

final_ann %>%
  fit(data = train_split) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

## Results  

We compare accuracy and roc_auc of the models on the training data.  

```{r}
collect_metrics(fit_tree) %>%
  bind_rows(collect_metrics(fit_nb)) %>%
  bind_rows(collect_metrics(fit_knn)) %>%
  bind_rows(collect_metrics(fit_svm)) %>%
  bind_rows(collect_metrics(fit_ann)) %>%
  filter(.metric == "accuracy") %>%
  mutate(model = c("decision tree", "naive Bayes", "knn", "svm", "ann")) %>%
  arrange(-.estimate) %>% 
  knitr::kable()
```

We look at the roc_auc curve.  

```{r}
#tree_rs %>%
#  unnest(.predictions) %>%
#  mutate(model = "decision tree") %>%
#  bind_rows(nb_rs %>%
#              unnest(.predictions) %>%
#              mutate(model = "naive Bayes")) %>%
#  bind_rows(knn_rs %>%
#              unnest(.predictions) %>%
#              mutate(model = "knn")) %>%
#  bind_rows(svm_rs %>%
#              unnest(.predictions) %>%
#              mutate(model = "svm")) %>%
#  bind_rows(ann_rs %>%
#              unnest(.predictions) %>%
#              mutate(model = "ann")) %>%
#  group_by(model) %>%
#  roc_curve(disease, .pred_0) %>%
#  ggplot(aes(
#    x = 1 - specificity,
#    y = sensitivity,
#    color = model
#  )) +
#  geom_line(size = 1.5) +
#  geom_abline(
#    lty = 2,
#    alpha = 0.5,
#    color = "gray50",
#    size = 1.2
#  )
```


***  

# Results  

***  

# Conclusion  

***  

# 