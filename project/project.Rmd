---
title: "CIS 635 Project"
author: "Chenfeng Hao"
date: "`r format(Sys.time(), tz = 'EST', '%b %d, %Y')`"
output: 
  pdf_document:
    toc: yes
    toc_depth: '2'
    dev: jpeg
  github_document:
  html_document:
    fig_caption: yes
    theme: lumen
    toc: yes
    toc_depth: 2
    df_print: kable
    toc_float:
      collapsed: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  cache.lazy = FALSE,
  dpi = 180,
  fig.width = 8,
  fig.height = 5
)
library(tidyverse)
library(tidymodels)
library(skimr)
library(caret)
theme_set(theme_minimal())
```
# Introduction  

In this project, we are given some demographic information and simple measurements of patients, and their test results for a certain disease. With these data, we practice some of the data mining techniques we learned through this class, including decision trees, naive Bayes, KNN, SVM, and ANN.  

***

# Preprocessing  

We first load the data into R.  

```{r}
personal_data <- read.table("projData1a.txt", header = T)
test_data <- read.table("projData1b.txt", header = T)
```

We take a look at the summary.  

```{r}
skim_without_charts(personal_data)
skim_without_charts(test_data)
```

We see a number of factor variables and "heartRate" has 738 missing values. For "weight", "height", and "heartRate", we do not see any extreme values. 

We convert the categorical variables to factors and merge the two sets by id.  

```{r}
personal_data_factored <-
  personal_data %>%
  mutate(across(c(ethnic:gender), as.factor))

test_data_factored <- 
  test_data %>% 
  mutate(disease = as.factor(disease))

merged_factored <-
  merge(personal_data_factored, test_data_factored, by = "id")
```

738 is less than 4% of the total records. We could either ignore the records with NA values for "heartRate", or we could replace these missing fields with values. As an exercise, we try replacing the missing fields with some appropriate measurements.  

We explore "heartRate" and its relationship with other variables in the dataset, in order to determine the proper replacements.  

```{r}
merged_factored %>%
  dplyr::select(c(age, weight:heartRate, testA:testE)) %>%
  pivot_longer(c(age, weight:height, testA:testE)) %>%
  ggplot(aes(heartRate, value)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  facet_wrap(~ name, scales = "free_y")

merged_factored %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::rearrange() %>%
  corrr::shave() %>%
  corrr::rplot(shape = 15, colours = c("darkorange","white","darkcyan"))

merged_factored %>%
  keep(is.numeric) %>%
  corrr::correlate() %>%
  corrr::network_plot(min_cor = 0.2)
```

From the plot for numeric variables, we do not see stronger trends except for "testE" and "weight." In other words, "heartRate" is not strongly associated with most numerical variables. We take a look at the categorical variables.  

```{r}
merged_factored %>%
  dplyr::select(c(ethnic:gender, heartRate, disease)) %>%
  pivot_longer(c(ethnic:gender, disease)) %>%
  ggplot(aes(heartRate, value, fill = value)) +
  geom_boxplot() +
  facet_wrap(~ name, scales = "free_y") 
```

In the plots for categorical variables and , we find the variables "ethnic" and "gender" are worth looking into, because their plots show greater variations of heartRate for different categories. We take a closer look at "gender" and "ethnic".  

```{r}
merged_factored %>%
  dplyr::select(c(ethnic, gender, heartRate)) %>%
  pivot_longer(c(ethnic, gender)) %>%
  ggplot(aes(heartRate, value, fill = value)) +
  geom_boxplot() +
  facet_wrap( ~ name, scales = "free_y") 

merged_factored %>%
  group_by(gender) %>%
  summarise(
    mean_heartRate = mean(heartRate, na.rm = TRUE),
    median_heartRate = median(heartRate, na.rm = TRUE)
  )

merged_factored %>%
  group_by(ethnic) %>%
  summarise(
    mean_heartRate = mean(heartRate, na.rm = TRUE),
    median_heartRate = median(heartRate, na.rm = TRUE)
  )
```

The mean "heartRate" values clearly differ between the genders, and between two ethnic groups and the other four.  

```{r}
merged_factored %>%
  ggplot(aes(ethnic, heartRate, fill = gender)) +
  geom_boxplot()

merged_factored %>%
  group_by(ethnic, gender) %>%
  summarise(
    mean_heartRate = mean(heartRate, na.rm = TRUE),
    median_heartRate = median(heartRate, na.rm = TRUE)
  ) 
```

We take a closer look and find 75 might be a good "heartRate" candidate for both genders of ethnic group 1 and 2. For the other ethnic groups, 60 looks like a good approximation for gender 0 and 67 for gender 1. Next we assign the miss values as such.  

```{r}
merged_factored_no_na <- merged_factored %>%
  na.omit()

filled_1 <- merged_factored %>%
  filter(ethnic == 1 | ethnic == 2) %>%
  filter(is.na(heartRate)) %>%
  mutate(heartRate = replace(heartRate, values = 75))

filled_2 <- merged_factored %>%
  filter(!(ethnic == 1 | ethnic == 2)) %>%
  filter(is.na(heartRate)) %>%
  filter(gender == 0) %>%
  mutate(heartRate = replace(heartRate, values = 60))

filled_3 <- merged_factored %>%
  filter(!(ethnic == 1 | ethnic == 2)) %>%
  filter(is.na(heartRate)) %>%
  filter(gender == 1) %>%
  mutate(heartRate = replace(heartRate, values = 67))

merged_factored_NAfilled <-
  rbind(merged_factored_no_na,
        filled_1,
        filled_2,
        filled_3)

project_data_df <-
  merged_factored_NAfilled %>%
  dplyr::select(-id)
```

The "recipe" package provides a number of methods for filling in missing values. We compare the results from our above operations with step_knnimpute, which replaces missing values using a k-nearest neighbor approach.  

```{r}
knn_impute <- recipe(disease ~ ., merged_factored) %>% 
  step_knnimpute(heartRate, neighbors = 3) %>% 
  prep %>% 
  juice

original_sum <- summary(merged_factored$heartRate)

transformed_sum <- summary(project_data_df$heartRate)

knn_impute_sum <- summary(knn_impute$heartRate)

sum_table <- bind_rows(original_sum,
                       transformed_sum,
                       knn_impute_sum)

bind_cols(sum_table, "source" = c("original",
                                  "transformed",
                                  "knn_impute"))
```

We see the difference between our replacement values for "heartRate" and that from "step_impute" is minimal for this dataset. 

***  

# Overview  

With data now cleaned and merged, we take another look at our data.  

```{r}
project_data_df %>%
  dplyr::select(c(age, weight:disease)) %>%
  pivot_longer(c(age, weight:testE)) %>%
  ggplot(aes(disease, value, col = disease)) +
  geom_boxplot() +
  facet_wrap( ~ name, scales = "free_y")

project_data_df %>%
  dplyr::select(c(ethnic:gender, disease)) %>%
  pivot_longer(ethnic:gender) %>%
  ggplot(aes(disease, value, col = value, fill = value)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ name, scales = "free_y")

project_data_df %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot() +
  geom_histogram(mapping = aes(x=value,fill=key), color="black") +
  facet_wrap(~ key, scales = "free")
```

The mean difference for "heartRate" seem to vary at a greater extent for disease. The numerical variables are mostly normal, except maybe "weight", which we would normalize for KNN, SVM, and ANN.    

***  

# Model Training

We start by dividing the data into a train and a test set. By default, this creates a 3 to 1 split between a training set and a testing set.  The "strata" parameter keeps the proportion of "disease" as in the whole set. The "vfold_cv" method creates 10 folds for resampling. We set "save_pred" to "TRUE" in order to retrieve metrics and predictions from the model.  

```{r}

set.seed(1)
data_split <- initial_split(project_data_df)
train_split <- training(data_split)
test_split <- testing(data_split)

set.seed(1)
train_cv <- vfold_cv(train_split)
model_control <- control_grid(save_pred = TRUE)
```

## Decision tree    

We set up the pipeline for training the decision tree model. Parameters "cost_complexity", "tree_depth", and "min_n" are tuned on a simple grid of values. The final model parameters are selected based on "accuracy".  

The training procedure for other models follows a similar process, adjusting for relevant parameters.    

### Training  

```{r}
tree_rec <- 
  recipe(disease ~ ., train_split)

tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_workflow <- workflow() %>% 
  add_recipe(tree_rec) %>% 
  add_model(tree_spec)

tree_grid <- tree_spec %>% 
  parameters() %>% 
  grid_regular(levels = 4)

doParallel::registerDoParallel()

set.seed(1)
tree_rs <- tune_grid(
  tree_workflow,
  resamples = train_cv,
  grid = tree_grid,
  control = model_control
)

final_tree <- 
  tree_workflow %>% 
  finalize_workflow(select_best(tree_rs, "accuracy"))

fit_tree <- 
  last_fit(final_tree, data_split)

doParallel::stopImplicitCluster()

fit_tree %>%
  collect_metrics()

fit_tree %>%
  collect_predictions() %>%
  conf_mat(truth = disease, .pred_class)

final_tree %>%
  fit(data = train_split) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

### Result  

We get very good "accuracy" from the decision tree model: 0.896. The importance plot shows "weight" has the biggest impact on prediction in this model, followed by "age", "heartRate", and "testE".

## Naive Bayes  

### Training

```{r}
nb_rec <- 
  recipe(disease ~ ., train_split)

nb_spec <- discrim::naive_Bayes(
  smoothness = tune(),
  Laplace = tune()
) %>%
  set_engine("klaR") 

nb_workflow <- workflow() %>% 
  add_recipe(nb_rec) %>% 
  add_model(nb_spec)

nb_grid <- nb_spec %>%
  parameters() %>%
  grid_regular(levels = 4)

doParallel::registerDoParallel()

set.seed(1)
nb_rs <- tune_grid(nb_workflow,
                   resamples = train_cv,
                   grid = nb_grid,
                   control = model_control)

final_nb <- 
  nb_workflow %>% 
  finalize_workflow(select_best(nb_rs, "accuracy"))

fit_nb <- 
  last_fit(final_nb, data_split)

doParallel::stopImplicitCluster()

fit_nb %>%
  collect_metrics()

fit_nb %>%
  collect_predictions() %>%
  conf_mat(truth = disease, .pred_class)
```

### Result  

The "accuracy" we get from the naive Bayes model is relatively low. Compared with the decision tree model, we get both more false positives and false negatives, as indicated in the confusion matrix. Unfortunately, we do not have an importance ranking for the model from the package we are using, same for KNN and SVM.    

## KNN  

### Training  

For KNN, we convert factors to dummy variables using "step_dummy", and center and scale numeric values using "step_normalize".  

```{r}
knn_rec <-
  recipe(formula = disease ~ ., data = train_split) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  prep()

knn_spec <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_workflow <- workflow() %>% 
  add_recipe(knn_rec) %>% 
  add_model(knn_spec)

knn_grid <- grid_regular(neighbors(),
                         weight_func(),
                         dist_power(),
                         levels = 2)

doParallel::registerDoParallel()

set.seed(1)
knn_rs <- tune_grid(knn_workflow,
                   resamples = train_cv,
                   grid = knn_grid,
                   control = model_control)

doParallel::stopImplicitCluster()

final_knn <- 
  knn_workflow %>% 
  finalize_workflow(select_best(knn_rs, "accuracy"))

set.seed(1)
fit_knn <- 
  last_fit(final_knn, data_split)

fit_knn %>%
  collect_metrics()

fit_knn %>% 
  collect_predictions() %>% 
  conf_mat(truth = disease, .pred_class)
```

### Result  

The "accuracy" we get from the KNN model is very close to that we get from the decision tree model, although still a bit lower.  

## SVM  

### Training  

For svm, we convert factors to dummy variables using "step_dummy", and center and scale numeric values using "step_normalize".  

Parsnip now only supports "poly" and "radial" kernels. We choose "poly" to practice with. The "cost" and "degree" values are obtained through previous tuning. Since it takes a long time to tune SVM models, instead of tuning hyperparameters as in other model training sessions, we use the parameter values directly.  

```{r}
svm_rec <-
  recipe(disease ~ ., train_split) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  prep()
  
svm_spec <- svm_poly(
  cost = 0.001,
  degree = 3
) %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_workflow <- workflow() %>% 
  add_recipe(svm_rec) %>% 
  add_model(svm_spec)

doParallel::registerDoParallel()

set.seed(1)

svm_fit <- 
  svm_spec %>% 
  fit(disease ~., data = train_split)

svm_fit_rs <- svm_workflow %>% 
  fit_resamples(train_cv)

# refit is done for accuracy retrieval consistent with other model training sessions
fit_svm <- svm_workflow %>%
  finalize_workflow(svm_fit_rs$.metrics) %>%
  last_fit(data_split)

svm_test_pred <-
  predict(svm_fit, test_split) %>%
  bind_cols(predict(svm_fit, test_split, type = "prob")) %>%
  bind_cols(test_split %>%
              dplyr::select(disease))

doParallel::stopImplicitCluster()

svm_test_pred %>%
 accuracy(truth = disease, .pred_class)

fit_svm %>% 
  collect_metrics()

fit_svm %>% 
  collect_predictions() %>% 
  conf_mat(truth = disease, .pred_class)
```

### Result 

We get relatively higher "accuracy" for SVM as well, partly because we are using a poly kernel.  

## ANN  

### Training

For ANN, we convert factors to dummy variables using "step_dummy", and center and scale numeric values using "step_normalize".  

```{r}
ann_rec <-
  recipe(disease ~ ., train_split) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_normalize(all_predictors(), -all_nominal()) %>% 
  prep()

ann_spec <- mlp(
  hidden_units = tune(),
  penalty = tune(),
  epochs = tune()
) %>%
  set_engine("nnet") %>%
  set_mode("classification")

ann_workflow <- workflow() %>% 
  add_recipe(ann_rec) %>% 
  add_model(ann_spec)

ann_grid <- ann_spec %>% 
  parameters() %>% 
  grid_regular(levels = 2)

doParallel::registerDoParallel()

set.seed(1)
ann_rs <- tune_grid(ann_workflow,
                   resamples = train_cv,
                   grid = ann_grid,
                   control = model_control)

final_ann <- 
  ann_workflow %>% 
  finalize_workflow(select_best(ann_rs, "accuracy"))

set.seed(1)
fit_ann <- 
  last_fit(final_ann, data_split)

doParallel::stopImplicitCluster()

fit_ann %>%
  collect_metrics()

fit_ann %>% 
  collect_predictions() %>% 
  conf_mat(truth = disease, .pred_class)

final_ann %>%
  fit(data = train_split) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

### Result  

We get the highest "accuracy" from the ANN model. It is harder to interpret the importance plot here, but "heartRate" is undoubtedly an important predictor variable for the disease.  

***  

# Results  

We compare the "accuracy" scores we get from all five models.   
```{r}
collect_metrics(fit_tree) %>%
  bind_rows(collect_metrics(fit_nb)) %>%
  bind_rows(collect_metrics(fit_knn)) %>%
  bind_rows(collect_metrics(fit_svm)) %>%
  bind_rows(collect_metrics(fit_ann)) %>%
  filter(.metric == "accuracy") %>%
  mutate(model = c("decision tree", "naive Bayes", "knn", "svm", "ann")) %>%
  arrange(-.estimate) %>% 
  knitr::kable()
```

In terms of "accuracy" in predicting the testing set ("test_split"), ANN performs the best, followed by decision tree, SVM, and KNN, with naive Bayes being the worst. However, of the top four, the differences between adjacent pairs are all close to 1%, suggesting they have very close prediction power for the current dataset. The naive Bayes model returns a much lower "accuracy". We take another look at the data and see why this may be the case.  

```{r}
project_data_df %>%
  dplyr::select(c(age, weight:heartRate, testA:disease)) %>%
  pivot_longer(c(age, weight:height, testA:testE)) %>%
  ggplot(aes(heartRate, value, col = disease)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ name, scales = "free_y")
```

In the plots of "heartRate" against all other numerical variables, it is not hard to find some approximate boundaries between the two classes of "disease". It may sometimes not be easy to get rectangular boundaries, but we see a relatively small number of clumps in most plots. We see a similar situation in the more complicated paired numerical variable plots.   

```{r}
library(GGally)

project_data_df %>%
  dplyr::select(c(age, weight:heartRate, testA:disease)) %>%
  ggpairs(
    mapping = ggplot2::aes(color = disease),
    lower = list(
      continuous = wrap("smooth", alpha = 0.3, size = 0.1),
      discrete = "blank",
      combo = "blank"
    ),
    upper = list(
      combo = wrap("box_no_facet", alpha = 0.5),
      continuous = wrap("cor", size = 4, alignPercent = 0.8)
    ) +
      theme(theme_minimal())
  )
```

Again, we see clumps and sometimes rectangular boundaries, but rarely a plot where the classes are centered apart and not overlapping. This suggests ANN would work best for this dataset. Decision tree may also have good performance here, probably not as good as ANN. And naive Bayes may not be a good classifier for this dataset. A linear kernel SVM may not perform as well, since all "age" plots does not seem to have a clear linear boundary. However, because we use a poly kernel here, we get an "accuracy" close to the top performers. KNN should have relatively worse performance, but we have tuned the hyperparameters and the number of variables may not be high enough to cause a steep performance drop.

***  

# Conclusion  

If we need to get prediction for this dataset, then ANN or decision tree may be the better choice. SVM with non-linear kernels or KNN could be a close second. If, however, we also want to interpret the model, then decision tree is clearly the best choice. 

Recall this plot.  

```{r}
final_tree %>%
  fit(data = train_split) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

It shows "weight", "age", "testE", and "heartRate" have the most impact in predicting "disease" in the model. This suggests one should be looking at these aspects when diagnosing a patient for this disease. Also important are "ethnic" and "gender". The result from "testE" is the most important for predicting the disease among all tests. The result from "testD" should not be relied on, at least not solely, for diagnosing the disease. 

In all, this has been a fruitful learning experience. I tried many things in "tidymodels" and learned about some of its advantages and limitations. And I very much enjoyed the struggle I had along the way.  

***  

# Appendix

## Reference  

https://www.tidymodels.org/  

https://juliasilge.com/  

https://github.com/andrew-couch/Tidy-Tuesday